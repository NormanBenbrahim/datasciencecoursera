---
title: "Week 1 Notes"
output:
  html_document:
    keep_md: yes
  word_document: default
---

## Probability

Given a random experiment, a probability measure is a ** population ** quantity that summarizes the randomness. Some rules of probability:

* The probability that nothing occurs is 0
* The probability that something occurs is 1
* The probability of something is 1 - the opposite outcome
* The probability of at least one of two (or more) things that cannot simultaneously occus (mutually exclusive) is the sum of their respective probabilities 
* If an event A implies the occurence of event B, then $p(A)<p(B)$

A random variable is a numberical outcome of an experiment, they can be discrete or continuous. Coin flips, if you recall, can be modeled with a binomial distribution, $p(x) = (1/2)^x(1/2)^{1-x}, x= \{0, 1\}$. Recall also that the probabilities are calculate as area under the curve or integral. 

The CDF if you remember is $F(X) = P(X \leq x)$. The survival function is $S(X) = P(X > x)$, so $F(X) = 1 - S(X)$. For your common distributions in `R`, `q` in front of the density name gives you quantiles. So `qnorm()` gives quantiles for the normal distribution, `qpois()` gives quantiles for the poisson distribution. `rnorm()` and `rpois()` gives you a `r`andom number generated by the normal and poisson distributions respectively. 

## Conditional probability

Let B be an even such that $P(B)>0$, then the conditional probability of an event A given that B has occured, is defined as $P(A | B) = \frac{P(A \cap B)}{P(B)}$

Bayes' rule: 
$P(B|A) = \frac{P(A|B)P(B)}{P(A|B)P(B) + P(A|B^c)P(B^c)}$

In the context of diagnostic tests; let $+$ and $-$ be the events that the result of a diagnostic test is pos or neg. Let $D$ and $D^c$ be the events that the subject has or doesnt have the disease. 

* Sensitivity $= P(+ | D)$
* Specificity $= P(- | D^c)$
* Positive predictive value $= P(D | +)$
* Negative predictive value $= P(D^c | -)$
* Prevalence of disease (absence of test) $= P(D)$

## Likelyhood ratios

Continuing with the disease and test case and Bayes' rule, we can equate two relationships:

1. $P(D | +) = \frac{P(+ | D)P(D)}{P(+ | D)P(D) + P(+ | D^c)(P(D^c)}$

2. $P(D^c|+) = \frac{P(+ | D^c)P(D^c)}{P(+ | D)P(D) + P(+ | D^c)(P(D^c)}$

Notice the two quantities have the same denominator. Dividing $1$ by $2$ yields

$\frac{P(+ | D)}{P(+ | D^c)} \cdot \frac{P(D)}{P(D^c)} = \frac{P(D | +)}{P(D^c | +)}$

What this tells us is the post test odds of D (RHS) is given by the pre-test odds of D (LHS2) times the diagnostic likelyhood ratio for a positive test result (LHS1). 

## Expected value

Defined as $E[x] = \sum xp(x)$

ex <- function (x, px) {
tot <- 0
for (i=1:length(x)) { 
tot <- tot + x[i]*px[i] }
tot}



